\documentclass[a4paper,11pt]{book}
\usepackage{import}
\usepackage{preamb}

\makeindex

\begin{document}

\input{head}
\newpage
\input{Title}
% \section{Blocks and Community structure}


\begin{subbox}{subbox}{}
\centering
\Large{\textbf{Community Structure}}
\end{subbox}




\begin{textbox}{Blocks and Communities: Definition}
The general idea of blocks and communities is that nodes of a network can be grouped together in homogeneous sets, based on the network topology. The problem of automatically discovering those groups is one of the most studied problem of network science, but also one of the most difficult to properly define.

\end{textbox}



\begin{textbox}{Partitions/Overlap}
We must differentiate two types of node grouping: 
\begin{enumerate}
    \item A \textbf{Partition} of a graph is a division of its nodes such as each of them belongs to one and only one group.
    \item Overlapping communities/blocks allow, on the contrary, nodes belonging to several groups. Unless specified differently, they also allow nodes to belong to no group.
\end{enumerate}
Algorithms looking for partitions are much more common than those searching for overlapping groups, due to the increased complexity of the later task. Overlapping community detection is, nevertheless, an active field of research.
\end{textbox}


\begin{textbox}{Community structure}
The idea of having a network structured in \textbf{communities} is defined as an analogy with communities in social networks. Communities are therefore defined (informally) as groups of nodes that are strongly connected between themselves (\textbf{high internal density}) and more weakly connected to the rest of the network (\textbf{low external density}).

This definition however cannot be translated unambiguously into a mathematical formulation. The problem of \textbf{community detection}, or community discovery, is therefore complex to define.
\end{textbox}



\begin{textbox}{Block structure}
The general idea of the block structure is that the probability to observe an edge between two nodes is a function of the blocks they belong to. Contrary to communities, no assumption is made \textit{a priori} about the respective values of those probabilities: \textbf{they can be high between nodes belonging to the same blocks or to different blocks}, and can \textbf{differ for each pair of block}.

% A  a random graph model, related to the ER random model, known as the \textbf{Stochastic block model}. 
\end{textbox}





\begin{textbox}{Definition}
\begin{tabular}{p{0.08\textwidth}|p{0.8\textwidth}}\scriptsize

$C$ & a \textit{community partition}, or, more generally, a set of set of nodes \\
$c_i$ & community $i$, a set of nodes \\
\end{tabular}
\end{textbox}


\begin{textbox}{Modularity}
The most famous quality function to measure the \textit{quality} of partitions is called the \textbf{Modularity}. Introduced in \footcite{girvan2002community}, it is defined for a partition $C$ and a graph $G$ as the difference between the fraction of observed internal edges and the expected fraction of internal edges if $G$ were rewired according to a configuration model, i.e., preserving the degrees of nodes.

More formally, 
\[
Q=\frac{1}{L}\sum_{i=1}^{|C|}(L_{i}-\frac{1}{2}K_i^2)
\]
with $L_{i}=L(H(c_i))$ the number of edges inside community $i$ and $K_i=\sum_{u \in c_i}k_u$ the sum of degrees of nodes in community $i$.

The original formulation of modularity, often found in the literature, is:
\[
Q=\frac{1}{2L}\sum_{uv}\left[ A_{uv}-\frac{k_uk_v}{2L}\right] \delta(c_u,c_v)
\]
with $\delta(c_u,c_v)$ the kronecker delta between communities, i.e., $\delta(c_u,c_v)=1$ if nodes $u$ and $v$ belong to the same community, 0 otherwise.


\end{textbox}

\begin{textbox}{Modularity: null model}
The modularity as expressed above compares the number of edges inside communities to the expected number of edges in a \textbf{null model}, i.e., a randomized version of the graph. In the original version, this null model is the \textbf{configuration model} (as easily recognized in the $\frac{k_uk_v}{2L}$ of the original formula).

Variants of the modularity have been proposed using different null models \footcite{jutla2011generalized}, for instance an ER null model, or a gravity model to take into account the effect of geographic distance \footcite{expert2011uncovering}
\end{textbox}


\begin{textbox}{Modularity: resolution limit}
It is important to remember that the Modularity is (only a) \textbf{quality function}, \textbf{not a definition} of the quality of communities. An important drawback of Modularity is known as the \textbf{resolution limit}\footcite{fortunato2007resolution}. It says that partitions of maximal modularity are biased toward a particular \textit{scale}, i.e., for a graph of a give size (\#nodes, \#edges), communities smaller than a certain size cannot be found. The typical example of this limit is the clique-ring structure (set of cliques connected by a single edge), in which the expected partition is to have one community by clique, while the solution of highest modularity put several cliques in the same community, when we increase the number of cliques.

\centering
\includegraphics[width=0.42\textwidth]{pics/cliqueRing20.png}
\includegraphics[width=0.4\textwidth]{pics/cliqueRing26.png}

Examples of networks with 20(left) and 26(right) cliques of size 5. Colors represent communities found by a modularity maximization algorithm.
\end{textbox}

\begin{textbox}{Modularity and random networks}
Another well known limitation of a Modularity maximization approach is that it finds communities with high scores in random networks: since it is not \textit{adjusted for chance}, random flucutations in a random network are mistaken for meaningful structure in the network.
\end{textbox}

\begin{textbox}{Multi-resolution Modularity}
A simple solution has been proposed to the limit of resolution, consisting in adding a resolution parameter $\lambda$ to \textit{tune} the desired resolution\footcite{reichardt2006statistical}, i.e., $(L_{i}-\frac{1}{2}K_i^2)$ becomes $(L_{i}-\lambda \frac{1}{2}K_i^2)$. It raises or shrinks the expected number of edges inside communities. It requires, however, to choose a proper value for $\lambda$, i.e., to choose arbitrarily a scale for communities.
\end{textbox}

\begin{textbox}{Mod. maximization: Girvan Newman}
Several of the most popular community detection algorithms have as objective to discover the partition of highest modularity. This is a difficult problem, and thus existing approaches are based on heuristics. 

The original method by Girvan and Newman \footcite{girvan2002community} first builds a dendrogram by iteratively removing the edge of highest betweenness. It is called a \textit{divise} approach: At the top of the dendrogram, there is a single community, then 2, 3, 4 etc., until each node is in its own community. Modularity is used as a criterium to \textit{cut} the dendrogram. 

\centering
\includegraphics[width=0.7\textwidth]{pics/dendrogram.jpg}

\end{textbox}

\begin{textbox}{Mod. maximization: Louvain method}
The Louvain method\footcite{blondel2008fast} is certainly the most used method for community detection. Its objective is to optimize the modularity using a greedy, agglomerative approach: 

\textbf{Step 1}: Optimizing modularity at a hierarchical level
\begin{itemize}
    \item Each node starts in its own community
    \item Repeat until convergence:
    \begin{enumerate}
        \item \textbf{FOR} each node, compute the gain in modularity of adding it to the community of each of its neighbors
        \item choose the decision that increases the most the modularity (the best decision can be to keep the node in the same community)
    \end{enumerate}
\end{itemize}

\textbf{Step 2}: Global algorithm
\begin{itemize}
    \item Repeat until convergence:
    \begin{enumerate}
        \item Optimize modularity for the current hierarchical level according to Step 1
        \item Move to a higher hierarchical level by computing an \textbf{induced network}: each community becomes a node, the weight of the edge between nodes/communities $i$ and $j$ corresponds to the number (sum of weights) of edges between nodes of $c_i$ and nodes of $c_j$.
    \end{enumerate}
\end{itemize}

The result of Louvain algorithm is a \textbf{hierarchy} of communities. 

\end{textbox}






% \begin{textbox}{Louvain Algorithm}
% Illustration of the Louvain algorithm from \footcite{traag2019louvain}
% \centering
% \includegraphics[width=0.99\textwidth]{pics/louvain.pdf}
% \end{textbox}




\begin{textbox}{Louvain method strengths and weaknesses}

The main reason explaining the popularity of the Louvain method to this day is its
   \textbf{scalability}: The algorithm is very scalable in practice on real graphs, for several reasons: 1)It is a greedy approach, 2) By checking only the interest of moving to neighbor's communities, it benefits from the sparsity of networks, 3)Modularity gains of a partition change can be computed locally, using its definition as a sum of independent values for each community.

Another advantage of the Louvain method is that results at lower hierarchical levels can naturally mitigate the problem of the resolution limit, for instance on the ring clique example, Louvain find each clique in its community at the first level, and only  in a second level yield the problematic partition.

However, it has also be shown in \footcite{traag2019louvain} that the greedy nature of the algorithm could lead to having counter-intuitive structures, such as disconnected communities. The authors of  the paper proceed to introduce a variant of the algorithm called Leiden, solving this problem.
\end{textbox}





\begin{textbox}{Infomap}
Infomap\footcite{rosvall2008maps} is a method based on an objective function different from the Modularity. Its objective is to \textbf{Minimize the description length of an average random walk} in the network, i.e. maximize the \textbf{compression} of the description of such a walk. More formally, the code length to minimize for partition $C$ is described as:
\[
H(C)=qH(\curvearrowright)+\sum_i^{|C|}p^iH(\circlearrowright_i)
\]
with $q$ the probability for a move to be between modules,$H(\curvearrowright)$ the amount of information (bits) required to encode a move between modules, $p^i$ the probability for a move to be inside community $i$ and $H(\circlearrowright_i)$ the amount of information required to encode a move inside community $i$.

A greedy optimization algorithm, for instance one similar in nature to the one of Louvain, can then be used to minimize this description length.

Compared with Modularity, the main advantage of this approach is that it does not systematically find communities in random networks. It is known also to suffer from a form of resolution limit.
Several improvements have been proposed, for instance to discover hierarchical partitions.
\end{textbox}




\begin{textbox}{Infomap Algorithm intuition}
Illustration of the intuition behind Infomap random walk compression. For a more accurate depiction, check the excellent illustration by the authors\footcite{rosvall2008maps}. The real encoding length is not computed explicitly, but estimated for an infinite random walk based on information theory principles.
\centering
\includegraphics[width=0.99\textwidth]{pics/infomap.pdf}
\end{textbox}





\begin{textbox}{Stochastic Block Models (SBM)}
A stochastic block model is a random graph model defined by:
\begin{tabular}{p{0.08\textwidth}|p{0.8\textwidth}}\scriptsize

% \begin{itemize}
 $k$ & number of blocks \\ 
 $b$ & $n\times 1$ vector such as $b_i$ describes the index of the block of node $i$. \\
 $E$ & $k\times k$ \textbf{stochastic block matrix}, such as $E_{ij}$ gives the number of edges between blocks $i$ and $j$ (or the probability to observe an edge between any pair of nodes chosen with one node in each of the two blocks).
% \end{itemize}
\end{tabular}
\end{textbox}

\begin{textbox}{SBM inference}
The objective of a community/block detection algorithm based on the SBM principle is thus to perform \textbf{SBM inference}, i.e., to find the parameters of the SBM that best explain the observed graph, usually in term of maximizing the likelihood. Said differently, we search --among a certain class of models-- the model that has the highest probability to generate the observed graph. Note that for an observed graph, for each partition in blocks $b$, there is a single block matrix $E$ that is relevant to consider, that can be found simply by counting the number of edges actually present between blocks in the graph.  

More formally, the objective is:
\[
b:=\argmax_{b} P(A|b)
\]

Note that with this formulation, it is not possible to infer the number of clusters $k$, since the trivial solution in which each node belongs to its own block, with $E=A$ has a maximal probabity (1) to generate the observed graph. The desired number of clusters is thus a necessary parameter of this type of SBM inference.
\end{textbox}






\begin{textbox}{SBM infering the number of blocks}
More recently, approaches\footcite{peixoto2019bayesian} have been proposed to infer also automatically the number of blocks. They adopt an approach from Information Theory called the Minimum Description Length (MDL), whose principle is to find the description which reduces the total cost of describing a graph, by minimizing both 1)The quantity of information needed to encode the graph, knowing that it is generated by a given model, and 2)The quantity of information needed to encode the model itself. Intuitively, a model with few blocks requires little information to be described, contrary to a model with many blocks. But a model with many blocks is more \textbf{constrained}, the graphs it generates are more \textit{specific}, and therefore can be described at a lesser cost, knowing the model.

\end{textbox}


\begin{textbox}{SBM infering blocks: equation}

More formally, we can decompose the probability of observing a graph and a model as:
\[
P (A, k, E, b) = P (A|k, E, b)P (k|E, b)P (E|b)P (b)
\]
with the last three probabilities being \textit{priors}.  Said differently, we can define the number of bits required to encode a model as $L = -log_2 P(k,b)$, the number of bits necessary to encode a graph knowing the model as $S = -log_2 P(A|k,b)$ and thus the total cost to minimize as $S+L$. 

The objective thus becomes:
\[
b:=\argmin_{b} - log_2 P(k,b) - log_2 P(A|k,b)
\]

\end{textbox}


\begin{textbox}{Variants of the SBM}
Group inference using SBM is a very active field of research, and many variants have been proposed, including degree-corrected, nested, Overlapping, Mixed membership SBM, etc.

An introduction to the state of the art can be found for instance in \footcite{lee2019review}.

A python library\footnote{https://graph-tool.skewed.de} exists to apply recent methods to observed graphs.
\end{textbox}



\begin{textbox}{Evaluation of Community structures}
Since there isn't a unique accepted definition of what are good communities, the evaluation of the quality of a partition or set of communities is not a trivial task. 

There are two main approaches:
\begin{itemize}
    \item \textbf{Internal evaluation} consists in using \textit{quality functions} (e.g., Modularity) to give a score for a pair partition/graph
    \item \textbf{External evaluation} consists in comparing a computed partition to a \textbf{ground truth} reference partition.
\end{itemize}

Internal evaluation can be used to evaluate the quality of communities found on a network of interest, while External evaluation is mostly used to asses the quality of algorithms on benchmarks. 
\end{textbox}

\begin{textbox}{Internal Evaluation - Global}
Objective quality function themselves can be understood as different \textbf{definitions of community structures}. While some methods try directly to optimize one of those quality functions, some other methods are based on different principles (e.g., clique-based communities, consensus reaching based on game-theory, etc.). Quality functions can therefore be used a posteriori to assess the quality of a partition.

Typical examples are:
\begin{itemize}
    \item \textbf{Modularity}
    \item \textbf{Description lenght}, as in Infomap or SBM
    \item \textbf{Surprise} \footcite{aldecoa2013surprise} evaluates the departure of the observed partition from the expected distribution of nodes and links into communities given a null model.
\end{itemize}

\end{textbox}


\begin{textbox}{Internal Evaluation - By community}

Some quality functions are defined at the level of \textbf{individual communities}, instead of having one score for the whole partition. Those individual scores can however be combined to provide a global score, for instance using a weighted average. Some of the most popular are\footnote{leskovec2010empirical}:
\begin{itemize}
    \item \textbf{Conductance}, the fraction of all stubs of nodes in the community that points outside of it
    \item \textbf{ODF}, Out Degree Fraction, the average for every node of its fraction of neighbors inside the community
    \item \textbf{Internal Transitivity}, the clustering coefficient inside the community
    \item \textbf{Scaled density}, the ratio of the node density to the total graph density
\end{itemize}


\end{textbox}







\begin{textbox}{External Evaluation}
Partitions obtained by a given method can be compared with a ground truth. This approach is used on real networks, with a ground truth coming from metadata (e.g., classes in a network of social interactions between students), and on synthetic networks, with communities known by construction.

Although this is still discussed in the literature, it is mostly accepted that the evaluation on real networks using this approach is problematic\footcite{peel2017ground}, because there is no guarantee that the labels used as ground truth are indeed related to the \textbf{topological structure} of the network, which is what communities are about.

Most popular methods for partitions comparisons are:
\begin{itemize}
\item \textbf{NMI}, Normalized Mutual Information, and its adjusted for chance variant, \textbf{AMI}.
\item \textbf{ARI}, Adjusted Rand Index
\end{itemize}
But more generally, any method for cluster comparison can be used\footcite{dao2020community}
\end{textbox}


\begin{textbox}{Overlapping communities}
For many types of networks, the real organization of networks is thought to be \textbf{overlapping}, i.e., each node can belong to several communities. Think for instance of your personal social network: some of your family members might also be part of a group of friends, or some of your friends from high school might also be part of your friends from university, which are otherwise distinct groups. 

Detecting overlapping clusters is considered harder than non-overlapping ones, for at least two reasons: the search space (number of possible solutions) is much larger (and even infinite), and defining what good communities are is even harder, since there isn't the natural limit for each edge to be either internal or external.

A large number of methods have nevertheless been proposed\footcite{xie2013overlapping}. Extensions of non-overlapping quality functions have been proposed, such as the overlapping Modularity \footcite{nicosia2009extending}, or overlapping NMI \footcite{mcdaid2011normalized}, but they are not as widely used as their more constrained counterparts.
\end{textbox}

\begin{textbox}{Other meso-scale structures}
Beyond the usual community structure, other types of network structural organizations have been proposed and studied. Some of the most widely known are:
\begin{itemize}
    \item \textbf{Link communities}, in which communities are defined as \textit{sets of links}. Searching for (non-overlapping) partitions of edges yield a structure in which nodes naturally belong to several groups, i.e., a community can corresponds to \textit{familial} edges, another to \textit{professional} edges, etc. (\cite{ahn2010link})
    \item \textbf{Fuzzy communities}, in which nodes belong to (often several) communities with a certain probability or strength (\cite{liu2010fuzzy})
    \item \textbf{Core-Periphery structure}, already defined when we introduced the notion of \textit{k-cores}
    \item \textbf{Nestedness}, corresponding to a network with a hierarchical organization such as elements with few connections tends to be connected to a subset of the neighbors of a \textit{parent} node. (\cite{pawar2014plant})
    \item \textbf{Spatial organization}, in which the probability of observing an edge between nodes depends on their distance. (\cite{barthelemy2011spatial})
\end{itemize}
\end{textbox}



\begin{textbox}{Meso-scale organization -1}
Examples of different types of organization that can be obtained using block structure


\centering

\includegraphics[height=0.45\textwidth]{pics/SBM/assortBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/assortativeG.png}

\textbf{(1) Assortative Structure}

\includegraphics[height=0.45\textwidth]{pics/SBM/anti_assortBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/anti_assortG.png}

\textbf{(2) Disassortative Structure
}

\includegraphics[height=0.45\textwidth]{pics/SBM/corePBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/corePG.png}

\textbf{(3) Core Periphery Structure
}

\includegraphics[height=0.45\textwidth]{pics/SBM/hierBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/hierG.png}

\textbf{(4) Hierarchical Structure
}



\end{textbox}


\begin{textbox}{Meso-scale organization -2}
\centering



\includegraphics[height=0.45\textwidth]{pics/SBM/uniformBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/uniformG.png}

\textbf{(5) Uniform/ Random
}



\includegraphics[height=0.45\textwidth]{pics/SBM/mixedBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/mixedG.png}

\textbf{(6) Mixed Structure
}




\includegraphics[height=0.45\textwidth]{pics/SBM/nestedBB.png}
\includegraphics[height=0.45\textwidth]{pics/SBM/nestedG.png}

\textbf{(7) Nested Structure}
%\includegraphics[width=0.99\textwidth]{pics/blocks.pdf}
\end{textbox}


\begin{textbox}{Going Further}

Surveys: (\cite{fortunato2010community}) (\cite{fortunato2016community})

On community detection approaches: (\cite{rosvall2019different})

On stochastic Block Models:\cite{funke2019stochastic}

Survey overlapping communities: (\cite{xie2013overlapping})

Community detection in dynamic networks (\cite{rossetti2018community})

On ground Truth and community detection: (\cite{peel2017ground})

Community detection in neuroscience (\cite{betzel2020community})



\end{textbox}



\input{tail}

\end{document}


